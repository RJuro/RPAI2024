{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd2zCJVnb-1z"
      },
      "source": [
        "# Building an Interactive Q&A Retriever\n",
        "\n",
        "This tutorial delves into the creation of a robust question and answer (Q&A) retrieval system using cutting-edge libraries in a notebook environment. Here's a sneak peek into the journey:\n",
        "\n",
        "1. **Library Installation:** Explore the installation of essential Python packages like `langchain`, `faiss-cpu`, `sentence_transformers`, and more, laying the foundation for building the Q&A system.\n",
        "\n",
        "2. **API Key Setup:** Learn how to retrieve and utilize user API keys, crucial for accessing external services like Together AI.\n",
        "\n",
        "3. **Q&A Retriever Setup:** Dive into setting up the Q&A retrieval system, utilizing advanced techniques such as text embeddings, vector databases, and language models.\n",
        "\n",
        "4. **Data Preparation:** Understand the process of loading and splitting text data, leveraging techniques like web scraping and text chunking for efficient indexing.\n",
        "\n",
        "5. **Example Data with Metadata:** Explore the creation of example datasets with attached metadata, essential for training and testing the Q&A system.\n",
        "\n",
        "6. **Retriever Construction:** Witness the construction of a powerful retriever, capable of searching and retrieving relevant documents based on user queries.\n",
        "\n",
        "7. **Language Model Integration:** Integrate advanced language models like Together AI's Open Hermes to generate accurate answers based on retrieved context.\n",
        "\n",
        "8. **Pipeline Creation:** Learn how to create a seamless pipeline or \"chain\" by combining retrievers, prompt templates, language models, and output parsers, enabling the system to handle user questions effectively.\n",
        "\n",
        "9. **Chain Invocation:** Experience the magic as the Q&A system springs to life, effortlessly answering sample questions based on retrieved context and language model predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-core langchain-community faiss-cpu tiktoken langchain-together sentence_transformers gradio -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NHUvclI9ALOt"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "import textwrap\n",
        "import os\n",
        "\n",
        "os.environ[\"TOGETHER_API_KEY\"] = userdata.get('TOGETHER_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      },
      "source": [
        "## Q&A-Retreiver\n",
        " This tutorial provides an overview of creating a question and answer (Q&A) retrieval system using various libraries in a notebook setting. Here are the main components and their functions:\n",
        "\n",
        "1. Installing required libraries: The code installs several Python packages, including `langchain`, `langchain-core`, `langchain-community`, `faiss-cpu`, `tiktoken`, `sentence_transformers`, and `Gradio`. These libraries are used to build the Q&A system.\n",
        "\n",
        "2. Loading user API key: The script retrieves an API key from the notebook's user data, which is likely needed for some external services like Together AI.\n",
        "\n",
        "3. Setting up the Q&A retriever: The tutorial demonstrates creating a question-answering system using different embedding models and vector databases. Here, `HuggingFaceEmbeddings` from the `langchain-community` library is used with the `intfloat/multilingual-e5-small` model for text embeddings. The embedded documents are stored in a FAISS (FAST AI Similarity Search) vector store.\n",
        "\n",
        "4. Loading and splitting data: The code loads text content from either a web page or a local file using `WebBaseLoader` or `TextLoader`. The loaded text is then split into smaller chunks using the `RecursiveCharacterTextSplitter` for efficient indexing.\n",
        "\n",
        "5. Creating an example data set with metadata: The split documents are used to create a vector store, which serves as the searchable database for the Q&A system.\n",
        "\n",
        "6. Building a retriever: A retriever is created from the FAISS vector store, allowing it to search and retrieve relevant documents based on user queries. In this case, the `search_kwargs` parameter specifies that three most similar documents should be returned for each query.\n",
        "\n",
        "7. Integrating a language model: The tutorial uses Together AI's Open Hermes model (`teknium/OpenHermes-2p5-Mistral-7B`) to generate answers based on the retrieved context. The `Together` class from the `langchain-together` library is used for this purpose.\n",
        "\n",
        "8. Creating a chain: A pipeline, or \"chain,\" is created by combining the retriever, prompt template, language model, and output parser. This chain allows the system to take a question as input and return an answer based on the retrieved context.\n",
        "\n",
        "9. Invoking the chain: Finally, the chain is invoked with a sample question (\"How did they start Instagram?\") to demonstrate how the Q&A system works. The answer is generated by combining the relevant documents retrieved from the vector store and using the language model to generate an appropriate response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IfCt8bhHNu9u"
      },
      "outputs": [],
      "source": [
        "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "#from langchain_together.embeddings import TogetherEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "#embeddings = OpenAIEmbeddings()\n",
        "#embeddings = TogetherEmbeddings(model=\"togethercomputer/m2-bert-80M-2k-retrieval\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-small\")\n",
        "\n",
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tZZH-NyTpgeR"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YkJerSHmn3iv",
        "outputId": "e16a5944-3a70-42fe-9b78-4c028becc4c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.document_loaders import TextLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qyt3sv3sopbh"
      },
      "outputs": [],
      "source": [
        "import bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P6HRkZB4omKP"
      },
      "outputs": [],
      "source": [
        "# Load, chunk and index the contents of the blog.\n",
        "loader = WebBaseLoader(\"https://lexfridman.com/sam-altman-2-transcript/\")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTDxLMs_vg8K"
      },
      "source": [
        "## Example data with metadata attached"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aHpTW4wcNsY0"
      },
      "outputs": [],
      "source": [
        "# get the wines in the store\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-DU2BD6sPJj"
      },
      "source": [
        "## Creating our retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GWW_t_MFsKC8"
      },
      "outputs": [],
      "source": [
        "from langchain_together import Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Cjo9-YU5rCnv"
      },
      "outputs": [],
      "source": [
        "llm = Together(\n",
        "    model=\"teknium/OpenHermes-2p5-Mistral-7B\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=512,\n",
        "    top_k=50,\n",
        "    # together_api_key=\"...\"\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40kasklQsfx1",
        "outputId": "ccf97cb4-1173-41e2-a58c-e9d73dbf3f5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Elon Musk lawsuit\\n\\nLex Fridman\\n(00:24:39) \\nOur mutual friend Elon sued OpenAI. What to you is the essence of what he’s criticizing? To what degree does he have a point? To what degree is he wrong?\\n\\n\\nSam Altman\\n(00:24:52) \\nI don’t know what it’s really about. We started off just thinking we were going to be a research lab and having no idea about how this technology was going to go. Because it was only seven or eight years ago, it’s hard to go back and really remember what it was like then, but this is before language models were a big deal. This was before we had any idea about an API or selling access to a chatbot. It was before we had any idea we were going to productize at all. So we’re like, “We’re just going to try to do research and we don’t really know what we’re going to do with that.” I think with many fundamentally new things, you start fumbling through the dark and you make some assumptions, most of which turned out to be wrong.', metadata={'source': 'https://lexfridman.com/sam-altman-2-transcript/', 'title': 'Transcript for Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - Lex Fridman', 'language': 'en-US'}),\n",
              " Document(page_content='Sam Altman\\n(00:33:51) \\nYeah, I really respect Elon and I hope that years in the future we have an amicable relationship.\\n\\n\\nLex Fridman\\n(00:34:05) \\nYeah, I hope you guys have an amicable relationship this month and just compete and win and explore these ideas together. I do suppose there’s competition for talent or whatever, but it should be friendly competition. Just build cool shit. And Elon is pretty good at building cool shit. So are you.\\n\\nSora\\n\\n\\n(00:34:32) \\nSo speaking of cool shit, Sora. There’s like a million questions I could ask. First of all, it’s amazing. It truly is amazing on a product level but also just on a philosophical level. So let me just technical/philosophical ask, what do you think it understands about the world more or less than GPT-4 for example? The world model when you train on these patches versus language tokens.', metadata={'source': 'https://lexfridman.com/sam-altman-2-transcript/', 'title': 'Transcript for Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - Lex Fridman', 'language': 'en-US'}),\n",
              " Document(page_content='Lex Fridman\\n(01:16:58) \\nI agree. I think Elon is a friend and he’s a beautiful human being and one of the most important humans ever. That stuff is not good.\\n\\n\\nSam Altman\\n(01:17:07) \\nThe amazing stuff about Elon is amazing and I super respect him. I think we need him. All of us should be rooting for him and need him to step up as a leader through this next phase.\\n\\n\\nLex Fridman\\n(01:17:19) \\nYeah. I hope he can have one without the other, but sometimes humans are flawed and complicated and all that kind of stuff.\\n\\n\\nSam Altman\\n(01:17:24) \\nThere’s a lot of really great leaders throughout history.\\n\\nGoogle and Gemini', metadata={'source': 'https://lexfridman.com/sam-altman-2-transcript/', 'title': 'Transcript for Sam Altman: OpenAI, GPT-5, Sora, Board Saga, Elon Musk, Ilya, Power & AGI | Lex Fridman Podcast #419 - Lex Fridman', 'language': 'en-US'})]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "retriever.get_relevant_documents('What do they say about Elon?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0ExKoMF0q9tz"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vi6DJV2iq_JM"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "had1f_a6rCIA",
        "outputId": "0148584c-a0ad-40fc-e4a6-be8b237731e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer: They say that Elon is a friend and a beautiful human being, and one of the most important humans ever. They also mention that Elon is a great leader and that all of them should be rooting for him to step up as a leader through the next phase.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chain.invoke(\"What do they say about Elon?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3DWhDwUrnXN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}